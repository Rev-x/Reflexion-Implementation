# -*- coding: utf-8 -*-
"""reflexion.py"""
import ollama

evaluator_schema = {
    "type": "object",
    "properties": {
        "query": {
            "type": "string",
            "description": "query asked to the AI"
        },
        "output": {"type": "string"},
        "overall_rating": {
            "type": "string",
            "description": "a single number between 0 to 100 that describes the overall rating of the response"
        },
        "scores": {
            "type": "object",
            "properties": {
                "relevance": {
                    "type": "string",
                    "description": "a single number between 0 to 100 that describes the relevance of the response"
                },
                "accuracy": {
                    "type": "string",
                    "description": "a single number between 0 to 100 that describes the accuracy of the response"
                },
                "completeness": {
                    "type": "string",
                    "description": "a single number between 0 to 100 that describes the completeness of the response"
                },
                "efficiency": {
                    "type": "string",
                    "description": "a single number between 0 to 100 that describes the efficiency of the response"
                },
                "technical_accuracy": {
                    "type": "string",
                    "description": "a single number between 0 to 100 that describes the technical accuracy of the response"
                },
                "emotional_score": {
                    "type": "string",
                    "description": "a single number between 0 to 100 that describes the emotional score of the response"
                },
                "creativity": {
                    "type": "string",
                    "description": "a single number between 0 to 100 that describes the creativity of the response"
                }
            }
        }
    }
}

output_schema = {
    "type": "object",
    "properties": {
        "task": {
            "type": "string",
            "description": "The original task or question."
        },
        "explanation": {
            "type": "string",
            "description": "an elaborated explanation or reasoning by the AI for the answer given"
        },
        "output": {
            "type": "string",
            "description": "The direct and precise answer generated by the AI, following the provided guidelines."
        }
    },
    "required": ["task", "output", "explanation"]
}

feedback_schema = {
    "type": "object",
    "properties": {
        "entire_feedback": {
            "type": "string",
            "description": "A detailed feedback generated by the AI, highlighting areas for improvement and specific suggestions, if the answer is satisfactory then provide with a good feedback and the answer is correct"
        }
    }
}

newprompt_schema = {
    "type": "object",
    "properties": {
        "oldtask": {
            "type": "string",
            "description": "the old prompt the AI generated the response from"
        },
        "feedback": {
            "type": "string",
            "description": "the feedback provided to the AI"
        },
        "oldresponse": {
            "type": "string",
            "description": "the response generated by the AI for the old response"
        },
        "explanation": {
            "type": "string",
            "description": "the explanation by the AI for the response generated"
        },
        "new_task": {
            "type": "string",
            "description": "new task improved based on the negative feedback to decrease the negative feedback further"
        }
    }
}


class ReflexionPipeline:
    def __init__(self, model_name):
        self.model_name = model_name
        self.short_term_memory = []
        self.long_term_memory = []
        self.prompt_template = """
        elaborate the explanation properly and clearly such so it gives clear explanation why the answer is generated by the AI
        Avoid prefacing your response with phrases like "The answer is" or "Here's the solution."
        Ensure your answer is complete and self-contained.
        If you cannot provide an answer, respond only with "Unable to answer."
        Fill in all the JSON schema formats concisely, precisely, and elaborately.
        """

    def ollama_response(self, prompt, model):
        response = ollama.chat(
            model=model,
            messages=[{'role': 'user', 'content': prompt}],
            stream=False,
            format='json',
            options = {'temperature':0.0}
        )
        return response['message']

    def generate_response(self, prompt):
        return self.ollama_response(prompt, self.model_name)

    def actor_model(self, task, context=None):
        if context:
            prompt = f"Context: {context}\nTask: {task}\n\n{self.prompt_template}"
        else:
            prompt = f"Task: {task}\n{self.prompt_template}\n\nthe answer is:"
        
        response = self.generate_response(prompt)
        response_json = self.generate_response(f"{response}\n\n{output_schema}")
        output = response_json['output']
        explanation = response_json['explanation']
        return output, explanation

    def evaluator_model(self, task, output, explanation):
        evaluation_prompt = f"""
        You are an expert evaluator AI tasked with critically assessing the quality and effectiveness of responses to given tasks. Analyze both the task requirements and the generated output, then fill the provided JSON SCHEMA with your evaluation results.

        Given:
        Task: ```{task}```
        Output: ```{output}```
        Explanation: ```{explanation}```

        Evaluation process:
        - Fill only a SINGLE numeric value between 1 and 10 for all score and rating fields in the schema.
        - Be strict on the scores; if any assumptions are made in the explanation, deduct points harshly.
        - If the explanation and the output are perfectly describing the answer and the reason, give it a high score.

        1. Analyze the task requirements, identifying key objectives, constraints, and specific criteria.
        2. Examine the output thoroughly, focusing on content, structure, and overall quality.
        3. Compare the output to the task requirements, considering:
            a. Relevance: Address of core task elements
            b. Completeness: Coverage of necessary points
            c. Accuracy: Correctness of information
            d. Clarity: Structure and coherence
            e. Creativity: Originality and innovation (if applicable)
            f. Efficiency: Conciseness and effectiveness
            g. Format: Adherence to specified requirements
            h. Language: Appropriate grammar, spelling, and tone
            i. Physical Relevance: Adherence to laws of physics and nature

        4. Evaluate additional task-specific factors as outlined in the original prompt.
        5. Assess strengths and weaknesses against task requirements.
        6. Consider potential improvements or alternative approaches.
        7. Synthesize your analysis into a holistic assessment.

        8. Assign scores (1-10) based on this scale:
            - 1: Poor (Fails to meet basic requirements)
            - 2: Below Average (Significant shortcomings)
            - 3: Fair (Meets some requirements)
            - 4: Satisfactory (Meets basic requirements)
            - 5: Above Average (Exceeds some requirements)
            - 6: Good (Meets most requirements effectively)
            - 7: Very Good (Exceeds most requirements)
            - 8: Excellent (Exceeds requirements with high quality)
            - 9: Outstanding (Exceptional quality and effectiveness)
            - 10: Perfect (Flawless execution, exceeds all expectations)

        9. Review your evaluation for accuracy and justification.
        10. Fill the provided JSON SCHEMA with your assessment results.

        Critical instructions:
        - Use only a SINGLE numeric value between 1 and 10 for all score and rating fields in the schema.
        - Ensure the overall_rating field (if present in the schema) is a number between 1 and 10.
        - Base your evaluation solely on the provided task and output.
        - Do not make assumptions unless explicitly stated in the task or output.
        - Apply relevant real-world principles of physics, mathematics, and logic.
        - Carefully check for adherence to known laws of physics and nature. Flag any violations or unrealistic scenarios in appropriate fields of the schema.
        - If the task involves hypothetical or fictional scenarios that intentionally deviate from natural laws, evaluate based on internal consistency rather than real-world physics.
        - Fill out all fields in the provided JSON SCHEMA completely and accurately.

        Additional check for physical relevance:
        - Assess whether the output adheres to established laws of physics and nature.
        - Identify any violations of physical laws or unrealistic scenarios not explicitly allowed by the task.
        - Evaluate the plausibility and feasibility of proposed solutions or ideas within the context of known scientific principles.
        - If the task involves speculative or fictional elements, ensure internal consistency and logical application of the established rules within that context.
        - Incorporate this assessment into relevant fields of the JSON SCHEMA, such as 'accuracy' or 'practical_applicability' if present.

        Final step:
        - Review the filled JSON SCHEMA to ensure all fields are completed accurately and in accordance with the evaluation guidelines.
        - Return only the filled JSON SCHEMA as your response, without any additional text or explanation.
        """
        
        response = self.generate_response(f"{evaluation_prompt}\n\n{evaluator_schema}")
        evaluation = response['overall_rating']
        return int(evaluation)

    def feedback_model(self, task, output, explanation):
        feedback_prompt = f"""
        You are an AI tasked with providing concise feedback on a given task and response. Fill out the provided JSON schema, focusing on negative aspects without making any assumptions not explicitly stated in the query. Only gravity can be assumed to exist.

        Given:
        Task: {task}
        Response: {output}
        Explanation: {explanation}

        Instructions:
        - Analyze the response and explanation in detail.
        - Provide feedback in the JSON schema.
        - Fill the "entire_feedback" with identified flaws and negative aspects.
        - Be strict and ensure no assumptions are made except for gravity.
        - Think step by step while comparing to evaluation.

        now provide the feedback in the json schema.
        """
        
        response = self.generate_response(f"{feedback_prompt}\n\n{feedback_schema}")
        feedback = response['entire_feedback']
        return feedback

    def update_actor_model(self, task, response, feedback):
        new_task_prompt = f"""
        You are an AI tasked with refining a task based on negative feedback to address identified flaws while maintaining the core intent. Analyze the original task and negative feedback to create an improved version that corrects mistakes.

        Given:
        Original Task: {task}
        Feedback: {feedback}

        make a refined task from the original task, refined task should not lose its original intent and meaning it should just be improvised according to the feedback

        Objectives:
        - Be logical, precise, and critical.
        - Review all provided information carefully.
        - Improve the task based on negative feedback while preserving its fundamental purpose.
        - Ensure the refined task:
            - Maintains clarity and focus.
            - Avoids ambiguity.
            - Is actionable and specific.
            - Incorporates improvements to prevent previous mistakes.

        JSON Schema Requirements:
        - Copy the original task into the "oldtask" field.
        - Copy the negative feedback into the "feedback" field.
        - Place the newly refined task into the "new_task" field.

        Final Step:
        - Return only the filled JSON schema without any additional text or explanation.
        """
        
        response = self.generate_response(f"{new_task_prompt}\n\n{newprompt_schema}")
        improved_task = response['new_task']
        return improved_task

    def memory_management(self, task, output, evaluation, feedback):
        self.short_term_memory.append((task, output, evaluation, feedback))

        evaluation_threshold = 9  # Changed to match the 1-10 scale

        if evaluation >= evaluation_threshold:
            self.long_term_memory.append((task, output, evaluation, feedback))

        if len(self.short_term_memory) > 5:
            self.short_term_memory.pop(0)

        if len(self.long_term_memory) > 20:
            self.long_term_memory.pop(0)

    def aggregate_memory(self):
        context = ""
        for mem in self.long_term_memory + self.short_term_memory:
            context += f"Task: {mem[0]}\nOutput: {mem[1]}\nEvaluation: {mem[2]}\nFeedback: {mem[3]}\n\n"
        return context

    def check_satisfaction(self, evaluation,criteria):
        return evaluation >= criteria # Changed to match the 1-10 scale

    def reflexion_loop(self, task, max_iterations, criteria):
        for iteration in range(max_iterations):
            context = self.aggregate_memory()

            output, explanation = self.actor_model(task, context)
            print("Output:", output)

            evaluation = self.evaluator_model(task, output, explanation)
            print("Evaluation:", evaluation)

            feedback = self.feedback_model(task, output, explanation)
            print("Feedback:", feedback)

            self.memory_management(task, output, evaluation, feedback)

            if self.check_satisfaction(evaluation, criteria):
                break

            task = self.update_actor_model(task, output, explanation, feedback)
            print("New Task:", task)

        return task, output, evaluation


# Example usage
model_name = 'your_model_name'  # Replace with your model name

reflexion = ReflexionPipeline(model_name)


task = "Bob is in the living room. He walks to the kitchen, carrying a cup. He puts a ball in the cup and carries the cup to the bedroom. In the bedroom, he turns the cup upside down. Then, he walks to the garden, where he puts the cup down. After that, he walks to the garage. Where is the ball now?"

final_task, final_output, final_evaluation = reflexion.reflexion_loop(task, max_iterations=5)

print("Final Task:", final_task)
print("Final Output:", final_output)
print("Final Evaluation:", final_evaluation)
